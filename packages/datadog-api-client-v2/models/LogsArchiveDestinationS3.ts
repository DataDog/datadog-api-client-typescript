/**
 * Unless explicitly stated otherwise all files in this repository are licensed under the Apache-2.0 License.
 * This product includes software developed at Datadog (https://www.datadoghq.com/).
 * Copyright 2020-Present Datadog, Inc.
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import { LogsArchiveDestinationS3Type } from "./LogsArchiveDestinationS3Type";
import { LogsArchiveIntegrationS3 } from "./LogsArchiveIntegrationS3";
import { ObjectSerializer } from "./ObjectSerializer";

/**
 * The S3 archive destination.
 */

export class LogsArchiveDestinationS3 {
  /**
   * The bucket where the archive will be stored.
   */
  "bucket": string;
  "integration": LogsArchiveIntegrationS3;
  /**
   * The archive path.
   */
  "path"?: string;
  "type": LogsArchiveDestinationS3Type;

  static readonly discriminator: string | undefined = undefined;

  static readonly attributeTypeMap: {
    [key: string]: { baseName: string; type: string; format: string };
  } = {
    bucket: {
      baseName: "bucket",
      type: "string",
      format: "",
    },
    integration: {
      baseName: "integration",
      type: "LogsArchiveIntegrationS3",
      format: "",
    },
    path: {
      baseName: "path",
      type: "string",
      format: "",
    },
    type: {
      baseName: "type",
      type: "LogsArchiveDestinationS3Type",
      format: "",
    },
  };

  static getAttributeTypeMap() {
    return LogsArchiveDestinationS3.attributeTypeMap;
  }

  static deserialize(data: { [key: string]: any }): LogsArchiveDestinationS3 {
    const res = new LogsArchiveDestinationS3();

    if (data.bucket === undefined) {
      throw new TypeError(
        "missing required attribute 'bucket' on 'LogsArchiveDestinationS3' object"
      );
    }
    res.bucket = ObjectSerializer.deserialize(data.bucket, "string", "");

    if (data.integration === undefined) {
      throw new TypeError(
        "missing required attribute 'integration' on 'LogsArchiveDestinationS3' object"
      );
    }
    res.integration = ObjectSerializer.deserialize(
      data.integration,
      "LogsArchiveIntegrationS3",
      ""
    );

    res.path = ObjectSerializer.deserialize(data.path, "string", "");

    if (data.type === undefined) {
      throw new TypeError(
        "missing required attribute 'type' on 'LogsArchiveDestinationS3' object"
      );
    }
    if (["s3", undefined].includes(data.type)) {
      res.type = data.type;
    } else {
      throw TypeError(`invalid enum value ${data.type} for type`);
    }

    return res;
  }

  static serialize(data: LogsArchiveDestinationS3): { [key: string]: any } {
    const attributeTypes = LogsArchiveDestinationS3.getAttributeTypeMap();
    const res: { [index: string]: any } = {};
    for (const [key, value] of Object.entries(data)) {
      if (!(key in attributeTypes)) {
        throw new TypeError(`${key} attribute not in schema`);
      }
    }
    if (data.bucket === undefined) {
      throw new TypeError(
        "missing required attribute 'bucket' on 'LogsArchiveDestinationS3' object"
      );
    }
    res.bucket = ObjectSerializer.serialize(data.bucket, "string", "");

    if (data.integration === undefined) {
      throw new TypeError(
        "missing required attribute 'integration' on 'LogsArchiveDestinationS3' object"
      );
    }
    res.integration = ObjectSerializer.serialize(
      data.integration,
      "LogsArchiveIntegrationS3",
      ""
    );

    res.path = ObjectSerializer.serialize(data.path, "string", "");

    if (data.type === undefined) {
      throw new TypeError(
        "missing required attribute 'type' on 'LogsArchiveDestinationS3' object"
      );
    }
    if (["s3", undefined].includes(data.type)) {
      res.type = data.type;
    } else {
      throw TypeError(`invalid enum value ${data.type} for type`);
    }

    return res;
  }

  public constructor() {}
}
